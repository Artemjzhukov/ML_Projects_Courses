# Деревья решений → Градиентный бустинг → Продвинутая оптимизация CatBoost.

## датасет [Employee dataset](https://www.kaggle.com/datasets/tawfikelmetwally/employee-dataset).

## датасет [Predict Students' Dropout and Academic Success](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success).

## Описание задачи и методов
Исследование посвящено сравнительному анализу алгоритмов машинного обучения на основе деревьев решений для задач регрессии и классификации.  
В проекте рассмотрены три основные группы методов: одиночные деревья решений, алгоритмы градиентного бустинга и методы оптимизации гиперпараметров.  
Эксперименты проводились на синтетических данных регрессии, данных о сотрудниках и образовательных данных студентов с использованием библиотек scikit-learn, XGBoost, LightGBM и CatBoost.

Анализ эффективности методов
Деревья решений с ограниченной глубиной показали устойчивость к переобучению на синтетических данных, при этом увеличение глубины сверх 5 уровней не приводило к значительному улучшению качества.  
Градиентный бустинг продемонстрировал лучшую способность к аппроксимации нелинейных зависимостей по сравнению с одиночными деревьями.

Сравнение алгоритмов бустинга выявило преимущество CatBoost в задачах с категориальными признаками, где разница в AUC достигает 0.091231 по сравнению с другими методами.  
Это объясняется эффективной обработкой категориальных переменных без предварительного кодирования через ordered target statistics.

Оптимизация гиперпараметров с помощью Optuna показала эффективность в улучшении качества моделей, при этом важность параметров распределилась следующим образом:  
learning_rate оказался наиболее значимым, затем max_depth и n_estimators. Ранняя остановка обучения позволила сократить вычислительные ресурсы без существенной потери качества.

В задаче бинарной классификации прогнозирования ухода сотрудников CatBoostClassifier показал AUC 0.879777 и F1 0.755682, что значительно превосходит результаты   
RandomForestClassifier (AUC 0.788446, F1 0.578512), GradientBoostingClassifier (AUC 0.786937, F1 0.577114), XGBClassifier (AUC 0.788559, F1 0.568528) и LGBMClassifier (AUC 0.785702, F1 0.566102).

В задаче многоклассовой классификации успеваемости студентов cравнение трех конфигураций CatBoostClassifier показало постепенное улучшение качества моделей.  
Базовая модель catb_stud_def достигла accuracy 0.7577, модель с ранней остановкой catb_stud_es - 0.7590, а оптимизированная с помощью Optuna модель catb_stud - 0.7667.  
Наибольшее улучшение наблюдается в классе "Enrolled", где precision вырос с 0.4968 до 0.5263, однако recall этого класса снизился с 0.3955 до 0.3778.  
Класс "Graduate" демонстрирует стабильно высокие показатели с recall 0.9031-0.9176, что указывает на хорошую идентификацию успешных студентов.

Выводы о применимости подходов
Одиночные деревья решений целесообразно использовать для начального анализа данных и построения интерпретируемых моделей, особенно при ограниченных вычислительных ресурсах.  
Градиентный бустинг демонстрирует превосходство в задачах с нелинейными зависимостями и требует тщательного подбора гиперпараметров для достижения максимальной эффективности.

CatBoost рекомендуется для задач с преобладанием категориальных признаков, где его встроенные механизмы обработки таких данных обеспечивают значительное преимущество перед другими алгоритмами.  
Для многоклассовой классификации с дисбалансом классов необходима дополнительная настройка метрик и весов классов.

Методы оптимизации гиперпараметров, такие как Optuna и RandomizedSearchCV, показывают высокую эффективность в автоматизации процесса настройки моделей.  
При этом выбор метода оптимизации зависит от размера пространства параметров и доступных вычислительных ресурсов.  
Для большинства практических задач достаточно 50-100 итераций поиска для достижения близких к оптимальным результатов.