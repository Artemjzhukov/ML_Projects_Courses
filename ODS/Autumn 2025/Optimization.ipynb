{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bef01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "027a96b4",
   "metadata": {},
   "source": [
    "### **Мультиколлинеарность и VIF**\n",
    "\n",
    "Еще одно из ключевых допущений линейной регрессии — отсутствие мультиколлинеарности между переменными. \n",
    "\n",
    "**Мультиколлинеарность** — это явление, при котором две или более независимые переменные в модели линейной регрессии сильно коррелируют друг с другом. Проще говоря, если изменение одной переменной тесно связано с изменением другой, то они являются мультиколлинеарными.\n",
    "\n",
    "**Последствия мультиколлинеарности:**\n",
    "\n",
    "* Оценки коэффициентов регрессии становятся очень чувствительными к небольшим изменениям в данных.\n",
    "\n",
    "* Усложняется оценка индивидуального влияния каждого фактора на зависимую переменную, так как их эффекты смешиваются.\n",
    "\n",
    "**Методы обнаружения мультиколлинеарности:**\n",
    "\n",
    "* Проверка парных корреляций. Если есть очень высокие значения (например, больше 0.8), это может указывать на наличие мультиколлинеарности.\n",
    "\n",
    "* Variance Inflation Factor (VIF). VIF оценивает, насколько дисперсия коэффициента регрессии увеличивается из-за мультиколлинеарности.\n",
    "\n",
    "Для каждого фактора $X_{j}$​ VIF вычисляется по формуле:\n",
    "\n",
    "$$\\mathrm{VIF}_{j}=\\frac{1}{1-R^2_{j}}$$\n",
    "\n",
    "где $R^2_{j}$​ — коэффициент детерминации модели, в которой строится регрессия $X_{j}$​ на все остальные факторы.\n",
    "\n",
    "**Интерпретация VIF:**\n",
    "\n",
    "* VIF = 1 — фактор не коррелирует с остальными.\n",
    "\n",
    "* 1 < VIF < 5 — фактор умеренно (допустимо) коррелирует с остальными.\n",
    "\n",
    "* VIF ≥ 5 — наличие мультиколлинеарности.\n",
    "\n",
    "**Методы устранения мультиколлинеарности:**\n",
    "\n",
    "* Удаление одной из коррелирующих переменных. Если две переменные мультиколлинеарны, можно удалить одну из них, оставив ту, которая более теоретически обоснована или имеет большее значение.\n",
    "\n",
    "* Объединение переменных. Создание новой переменной, которая является комбинацией мультиколлинеарных переменных.\n",
    "\n",
    "* Использование моделей с регуляризацией. Модели Ridge и LASSO разработаны для работы с мультиколлинеарностью путем добавления штрафа, который уменьшает влияние коррелированных переменных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c862864",
   "metadata": {},
   "source": [
    "### **Гетероскедастичность и тест Уайта**\n",
    "\n",
    "**Гетероскедастичность** — это нарушение одной из ключевых предпосылок классической линейной регрессии, которое заключается в том, что дисперсия (разброс) остатков модели не является постоянной для всех наблюдений.\n",
    "\n",
    "**Последствия гетероскедастичности:**\n",
    "\n",
    "* Неэффективность МНК-оценок — оценки, полученные с помощью метода наименьших квадратов (МНК), не являются наиболее точными.\n",
    "\n",
    "* Смещённость и несостоятельность ковариационной матрицы МНК-оценок. Это приводит к тому, что статистические выводы о качестве полученных оценок (t-тесты, F-тест и доверительные интервалы) могут быть неадекватными.\n",
    "\n",
    "**Выявление гетероскедастичности:**\n",
    "\n",
    "* Графики остатков регрессии. В первом приближении наличие гетероскедастичности можно выявить на графиках остатков регрессии по некоторым переменным, по оцененной зависимой переменной или по номеру наблюдения: разброс точек может меняться в зависимости от значения этих переменных. \n",
    "\n",
    "* Статистические тесты Уайта, Голдфелда-Квандта, Бройша-Пагана, Парка, Глейзера, Спирмена. \n",
    "\n",
    "**Тест Уайта** — это один из наиболее часто применяемых статистических тестов для обнаружения гетероскедастичности, преимущество которого заключается в том, что он не требует заранее предполагать, от чего именно зависит дисперсия ошибки. Суть этого теста заключается в проверке, существует ли статистически значимая связь между квадратами остатков исходной модели и факторами, а также их квадратами и попарными произведениями.\n",
    "\n",
    "**Тест Уайта:**\n",
    "\n",
    "1. Оценивается исходная регрессия и вычисляются её остатки ($e$).\n",
    "\n",
    "2. Строится вспомогательная регрессия квадратов остатков ($e^2$) на исходные признаки, их квадраты и попарные произведения.\n",
    "\n",
    "3. Проверяется гипотеза:\n",
    "\n",
    "    * $H_0$: гетероскедастичности нет (гомоскедастичность, дисперсия ошибок постоянна).\n",
    "    \n",
    "    * $H_1$: гетероскедастичность присутствует."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ff87c",
   "metadata": {},
   "source": [
    "Проверьте наличие гетероскедастичности в данных модели `reg_housing` с помощью теста Уайта на уровне значимости 5% ([het_white](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.het_white.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполните тест Уайта\n",
    "\n",
    "housing_white_test = het_white(reg_housing_res, \n",
    "                               sm.add_constant(X_housing_train_scaled))\n",
    "\n",
    "housing_white_test_result = pd.DataFrame(\n",
    "    np.round(housing_white_test, 6), \n",
    "    index=['Test Statistic', 'Test Statistic p-value', 'F-Statistic', 'F-Test p-value'], \n",
    "    columns=['Value']\n",
    ")\n",
    "housing_white_test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea1498",
   "metadata": {},
   "source": [
    "[RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) — метод подбора оптимальных гиперпараметров модели, суть которого состоит в переборе случайных комбинаций значений параметров из заданных диапазонов или распределений вместо полного перебора всех возможных комбинаций (как это реализовано в GridSearchCV).\n",
    "\n",
    "**RandomizedSearchCV и GridSearchCV**:\n",
    "\n",
    "* **RandomizedSearchCV** выбирает комбинации случайно. Это снижает вычислительные затраты и позволяет находить приемлемые (иногда лучшие) гиперпараметры при меньших вычислительных затратах. **Параметр n_iter** указывает число случайных комбинаций гиперпараметров, которые будут протестированы в процессе подбора.\n",
    "\n",
    "* **GridSearchCV** перебирает все комбинации значений гиперпараметров. Это гарантирует нахождение результата внутри сетки, но требует много времени, особенно при большом числе параметров или возможных значений.\n",
    "\n",
    "Подробнее можно изучить по **ссылке:**\n",
    "\n",
    "* [Рандомизированная оптимизация параметров | scikit-learn.ru](https://scikit-learn.ru/stable/modules/grid_search.html#randomized-parameter-search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a480bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_iter = 50\n",
    "scoring = 'roc_auc'\n",
    "cv = 5\n",
    "\n",
    "cv_ran = RandomizedSearchCV(\n",
    "    estimator=model(random_state=RANDOM_STATE),\n",
    "    param_distributions=params,\n",
    "    n_iter=n_iter,\n",
    "    scoring=scoring,\n",
    "    cv=cv,\n",
    "    random_state=RANDOM_STATE\n",
    "    n_jobs=-1, # Может ускорить вычисления за счёт параллелизма, не влияет на результат\n",
    "    refit=True # Переобучает модель на всей выборке после подбора гиперпараметров (по умолчанию True, можно не указывать)\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "cv_best = cv_ran.best_estimator_\n",
    "\n",
    "# Выведите оптимальные гиперпараметры обучения tree_titanic и номер итерации, на котором они были достигнуты\n",
    "\n",
    "print(f'Оптимальные параметры DecisionTreeClassifier на итерации {cv_ran.best_index_}: {cv_ran.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b324f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b685d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': [100, 150, 200, 250],\n",
    "    'learning_rate': [0.1, 0.2, 0.3, 0.4],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "cv = 5\n",
    "\n",
    "cv_grid = GridSearchCV(\n",
    "    estimator=model(random_state=RANDOM_STATE),\n",
    "    param_grid=params,\n",
    "    cv=cv,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "cv_grid.fit(X, y)\n",
    "\n",
    "# Выведите оптимальные гиперпараметры по результатам оптимизации\n",
    "\n",
    "print(f'Оптимальные параметры: {cv_grid.best_params_}')\n",
    "print(f'Лучший score: {cv_grid.best_score_:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7d0d7",
   "metadata": {},
   "source": [
    "### **Cost-Complexity Pruning**\n",
    "\n",
    "**Cost-Complexity Pruning** (CCP) — это метод борьбы с переобучением в деревьях решений, суть которого состоит в нахождении оптимального баланса между точностью модели и сложностью дерева.\n",
    "\n",
    "Общий критерий оптимизации:\n",
    "\n",
    "$$R_{\\alpha}(T)=R(T)+\\alpha|T|$$\n",
    "\n",
    "где $R(T)$ — ошибка дерева (к примеру, MSE/MAE для регрессии или Gini/Entropy для классификации), $|T|$ — количество листьев в дереве, $\\alpha$ — параметр регуляризации.\n",
    "\n",
    "Cost-Complexity Pruning в sklearn для деревьев решений (DecisionTreeClassifier и DecisionTreeRegressor) реализуется с помощью **параметра ccp_alpha**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# С помощью tree_housing рассчитайте все возможные значения ccp_alpha, используя метод cost_complexity_pruning_path\n",
    "\n",
    "ccp_alphas = tree_housing.cost_complexity_pruning_path(X_housing_train, y_housing_train).ccp_alphas\n",
    "\n",
    "# Подберите оптимальное значение ccp_alpha с помощью GridSearchCV\n",
    "# Не забудьте зафиксировать RANDOM_STATE\n",
    "\n",
    "params = {'ccp_alpha': ccp_alphas}\n",
    "scoring='neg_mean_squared_error'\n",
    "cv = 5\n",
    "\n",
    "# Обучите дерево tree_housing_ccp с оптимальным параметром ccp_alpha\n",
    "# Не забудьте зафиксировать RANDOM_STATE\n",
    "\n",
    "tree_housing_ccp = DecisionTreeRegressor(\n",
    "    ccp_alpha=cv_tree_housing_ccp.best_params_['ccp_alpha'],\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "tree_housing_ccp.fit(X_housing_train, y_housing_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a720fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используя SequentialFeatureSelector, подберите на обучающей выборке оптимальный набор признаков\n",
    "# Не забудьте зафиксировать RANDOM_STATE\n",
    "\n",
    "bankr_sfs = SequentialFeatureSelector(\n",
    "    estimator=LogisticRegression(random_state=RANDOM_STATE, solver='liblinear', class_weight='balanced'), \n",
    "    k_features='best', \n",
    "    forward=True, \n",
    "    floating=False,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ").fit(X_bankr_train, y_bankr_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d104ac4",
   "metadata": {},
   "source": [
    "### **Early Stopping**\n",
    "\n",
    "Ранняя остановка (Early Stopping) — это универсальный и широко распространённый метод регуляризации, который позволяет эффективно предотвращать переобучение моделей. Суть метода заключается в остановке обучения модели до завершения всех запланированных итераций в случае, если прогнозные способности модели на валидационной выборке перестают улучшаться или начинают ухудшаться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd22ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определите целевую функцию objective для оптимизации параметров с помощью optuna\n",
    "# Не забудьте фиксировать random_state, где это возможно\n",
    "\n",
    "def objective(trial, X, y, cat_features, cv=4, random_state=None):\n",
    "    \"\"\"\n",
    "    Целевая функция для оптимизации гиперпараметров CatBoostClassifier с помощью optuna.\n",
    "\n",
    "    Аргументы:\n",
    "        trial (optuna.trial.Trial): Объект trial для предложения гиперпараметров.\n",
    "        X (pandas.DataFrame): Таблица с признаками.\n",
    "        y (array-like): Массив значений целевой переменной.\n",
    "        cat_features (list[str]): Список с категориальными признаками.\n",
    "        cv (int): Количество фолдов для стратифицированной кросс-валидации. По умолчанию — 5.\n",
    "        random_state : (int|None): Сид для фиксирования случайного состояния. По умолчанию — None (не фиксировать).\n",
    "    \"\"\"\n",
    "    params = {\n",
    "    'learning_rate': trial.suggest_float('learning_rate', 0.05, 1, log=True), # Темп обучения: float между 0.05 и 1 в логарифмическом масштабе    \n",
    "    'max_depth': trial.suggest_int('max_depth', 1, 8), # Максимальная глубина деревьев: int между 1 и 8\n",
    "    'n_estimators': trial.suggest_int('n_estimators', 100, 800), # Количество деревьев в ансамбле: int между 100 и 800\n",
    "    'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.05, 1.0), # Доля признаков, используемых для построения каждого уровня дерева: float между 0.05 и 1.0\n",
    "\n",
    "    'cat_features': cat_features,\n",
    "    'eval_metric': 'Accuracy',\n",
    "    'random_state': random_state, \n",
    "    'verbose': False\n",
    "    }\n",
    "\n",
    "    early_stopping_rounds = trial.suggest_int('early_stopping_rounds', 20, 100) # Критерий остановки: int между 20 и 100\n",
    "    \n",
    "    accuracy_scores = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model = CatBoostClassifier(**params).fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            early_stopping_rounds=early_stopping_rounds\n",
    "        )\n",
    "        accuracy_scores.append(model.best_score_['validation']['Accuracy']) # val\n",
    "    \n",
    "    return np.mean(accuracy_scores)\n",
    "\n",
    "# Оптимизируйте гиперпараметры модели с помощью optuna (sampler — TPESampler)\n",
    "# Для подбора гиперпараметров используйте train\n",
    "# Не забудьте зафиксировать RANDOM_STATE (в seed TPESampler)\n",
    "\n",
    "n_trials = 60 # Количество тестируемых комбинаций параметров\n",
    "cv = 4 # Количество фолдов при кросс-валидации\n",
    "\n",
    "stud_sampler = TPESampler(seed=RANDOM_STATE)\n",
    "\n",
    "stud_study = optuna.create_study(\n",
    "    direction='maximize', # Максимизация accuracy\n",
    "    sampler=stud_sampler,\n",
    "    study_name='CatBoostClassifier'\n",
    ")\n",
    "\n",
    "stud_study.optimize(lambda trial: objective(trial, X_stud_train, y_stud_train, cat_features=stud_cat_feat, cv=cv, random_state=RANDOM_STATE), \n",
    "    n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907c03a",
   "metadata": {},
   "source": [
    "### **Вероятностные методы оптимизации гиперпараметров**\n",
    "\n",
    "Вероятностные методы оптимизации гиперпараметров — это итерационные методы, которые позволяют находить оптимальные гиперпараметры обучения моделей быстрее и точнее, чем Grid Search и Randomized Search, за счет использования вероятностной модели целевой функции. \n",
    "\n",
    "На каждой итерации метод оценивает, в какой следующей точке с наибольшей вероятностью будет достигнуто улучшение текущей оценки оптимума. Вероятностные методы особенно полезны при работе со сложными многомерными пространствами гиперпараметров.\n",
    "\n",
    "**Преимущества вероятностных методов перед Grid Search и Randomized Search:**\n",
    "\n",
    "* Каждая итерация использует информацию, полученную на предыдущих итерациях.\n",
    "\n",
    "* Вероятностные методы способны моделировать внутренние зависимости между гиперпараметрами.\n",
    "\n",
    "* Вероятностные методы позволяют достичь более высокого качества, если было выполнено достаточное количество итераций.\n",
    "\n",
    "* Гибкость. Вероятностные методы способны работать с непрерывными, дискретными и категориальными параметрами.\n",
    "\n",
    "**Основным недостатком** вероятностных методов является высокая вычислительная сложность по сравнению с Grid Search и Randomized Search.\n",
    "\n",
    "Одним из основных вероятностных методов является TPE (Tree-structured Parzen Estimator). TPE реализован в двух наиболее популярных библиотеках для оптимизации гиперпараметров: Optuna и Hyperopt.\n",
    "\n",
    "Подробнее можно изучить по **ссылкам:**\n",
    "\n",
    "* [Подбор гиперпараметров | education.yandex.ru](https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov).\n",
    "\n",
    "* [Optuna vs Hyperopt: Which Hyperparameter Optimization Library Should You Choose? | eptun.ai](https://neptune.ai/blog/optuna-vs-hyperopt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f38fbeb",
   "metadata": {},
   "source": [
    "# regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aec753",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_diab_ece_groups_const = sm.add_constant(X_diab_ece_groups)\n",
    "ols_ece_diab = OLS(y_diab_ece_groups, X_diab_ece_groups_const).fit()\n",
    "print(ols_ece_diab.summary())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
